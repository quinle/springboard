{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba05f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85db90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df_train_label = pd.read_csv('/Users/toanngo/Documents/GitHub/capstone 2 data/raw data /train_labels.csv')\n",
    "df_test = pd.read_csv('/Users/toanngo/Documents/GitHub/capstone 2 data/raw data /test_data.csv', nrows = 10**5)\n",
    "df_train = pd.read_csv('/Users/toanngo/Documents/GitHub/capstone 2 data/raw data /train_data.csv', nrows = 10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2532d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the train data train_label\n",
    "#Note that D = Delinquency, S = Spend, P = Payment, B = Balance, R = Risk\n",
    "df_train_label.head()\n",
    "df_train_label.info()\n",
    "df_train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the test data\n",
    "df_test.head()\n",
    "df_test.info()\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60957adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the train data\n",
    "df_train.head()\n",
    "df_train.info()\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e82ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns == df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe832e48",
   "metadata": {},
   "source": [
    "The train data has same columns with the test data with different year on S_2. It seems that American express is trying to predict behavior of customer in 2019 from the behavior of customer from previous years. Moreover, because they have same columns, the data cleaning we apply to train data can be applied to test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8edea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if customer_ID is unique for the data sets\n",
    "df_train_label.customer_ID.is_unique\n",
    "df_test.customer_ID.is_unique\n",
    "df_train.customer_ID.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c9d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the customer_ID is non-unique for test and train data, we figure out what is the issue\n",
    "df_test.customer_ID.value_counts()\n",
    "df_train.customer_ID.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaaebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We investigate the first customer_ID from train set'0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a'\n",
    "df_train[df_train.customer_ID == '0000099d6bd597052cdcda90ffabf56573fe9d7c79be5fbac11a8ed792feb62a']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f765f6b",
   "metadata": {},
   "source": [
    "This customer had 13 transactions from 2017 to 2018 which might contain important info for fraud detection. Therefore, we can not drop the duplicate customer_ID for train and test table. We now investigate further train and test table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate df_train and df_test\n",
    "categorical_train = df_train.select_dtypes(include=['object']) #select categorical columns\n",
    "categorical_test = df_test.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45970424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.D_63.value_counts()\n",
    "df_train.D_64.value_counts()\n",
    "df_test.D_63.value_counts()\n",
    "df_test.D_64.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601bfb5",
   "metadata": {},
   "source": [
    "Since there are only 2 categorical columns and they have limited distinct values, we can consider using one hot encoding when we build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing value, note that these tables have 100000 rows each\n",
    "missing_train = df_train.isna().sum().sort_values(ascending = False).to_frame('missing_count')\n",
    "missing_test = df_test.isna().sum().sort_values(ascending = False).to_frame('missing_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of columns we will drop\n",
    "to_drop_train = missing_train[missing_train['missing_count'] > 95000] #we drop if 95% or more is NaN\n",
    "to_drop_test = missing_test[missing_test['missing_count'] > 95000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(to_drop_train)\n",
    "print(to_drop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338dc46",
   "metadata": {},
   "source": [
    "Although there is some small difference in the order, the lists of dropped columns are the same for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e8037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unneccessary columns\n",
    "to_drop = ['D_87', 'D_88', 'D_108', 'D_111', 'D_110', 'B_39', 'D_73', 'B_42', 'D_134', 'D_138', 'D_135', 'D_136', 'D_137']\n",
    "df_train = df_train.drop(to_drop, axis = 1)\n",
    "df_test = df_test.drop(to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fdd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for total missing values of columns\n",
    "plt.figure(figsize = (20,5))\n",
    "plt.plot(missing_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe5c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill NaN value with 0\n",
    "df_train.fillna(0, inplace = True)\n",
    "df_test.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join with label, note that we only have label for train data set so we join df_train with label only\n",
    "df_train = df_train.set_index('customer_ID').join(df_train_label.set_index('customer_ID'), on = 'customer_ID', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for later use\n",
    "df_train.to_csv(path_or_buf='~/Documents/GitHub/capstone 2 data/processed data /df_train.csv', index=False)\n",
    "df_test.to_csv(df_train.to_csv(path_or_buf='~/Documents/GitHub/capstone 2 data/processed data /df_test.csv', index=False)\n",
    ", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd87dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
